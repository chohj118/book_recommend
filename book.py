import streamlit as st
import pandas as pd
import numpy as np
import re
import matplotlib.pyplot as plt
import seaborn as sns
import tomotopy as tp
import sys
import logging
from tqdm import tqdm
import koreanize_matplotlib

# í˜ì´ì§€ ì œëª© ì„¤ì •
st.set_page_config(
    page_icon='ğŸ“š',
    page_title='ì´book ì–´ë•Œ?',
    layout='wide'
)

# í—¤ë”ì™€ ì„œë¸Œí—¤ë” ì„¤ì •
st.header('ë„ì„œì¶”ì²œì‹œìŠ¤í…œ')
st.subheader('ì¶”ì²œ')

# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
df = pd.read_csv('data/token.csv')
st.dataframe(df)

df['ì„¤ëª…'] = df['ì„¤ëª…_okt']
df = df[['ìˆœìœ„', 'mean', 'ISBN', 'ìƒí’ˆëª…', 'ì €ì', 'ì¶œíŒì‚¬', 'ì„¤ëª…', 'ê´€ë¦¬ë¶„ë¥˜']]
# ì„¤ëª… ì»¬ëŸ¼ ì •ê·œí‘œí˜„ì‹
df['ì„¤ëª…'] = df["ì„¤ëª…"].str.replace('[^ê°€-í£ã„±-ã…ã…-ã…£a-zA-Z ]', "", regex=True)

# ë¶ˆìš©ì–´
stopwords = ["í•©ë‹ˆë‹¤", "ì…ë‹ˆë‹¤", "ìˆë‹¤", "ì´ ì±…ì€", "ì´", 'ì ', 'ê¸°', 
        "ê·¸", "ë² ìŠ¤íŠ¸ì…€ëŸ¬", "ë“±", "ê²ƒì„", "ì €ìê°€", "ëœë‹¤", 
        "ìˆëŠ”", "ìœ„í•´", "ì±…ì€", "ê·¸ë¦¬ê³ ", "ë…„", "ëŒ€í•œ", "í†µí•´", "ì €ìëŠ”", "ìˆ˜ ìˆë„ë¡", "ê²ƒì´ë‹¤", "ì„", "ê²ƒ", "ë‚´", "ì˜", "ê°œì˜", "ë”", "ë ", "ê°œ", 
        "ë°", "ì—¬", "ê°", "ë•Œ", "ì¤‘", "ë•Œ", "ì „", "ì€", "ê±´", "ì–´ë–»ê²Œ", "ë¥¼", "ëŠ”", "ë‚´ê°€", "ë§Œ", 'í• ', 'í›„', 
        "í›„", "ì„", "ì—", "ë¡œ", "ëŒ€í•´", "ëª¨ë“ ", "í•œ", "ëœ", "ë‹¤", "ë˜ì—ˆë‹¤", "ê°œì˜", "ë”", "ë ", "ê°œ", 
        "ë°", "ì—¬", "ê°", "ë•Œ", "ì¤‘", "ë•Œ", "ì „", "ì€", "ê±´", "ì–´ë–»ê²Œ", "ë¥¼", "ëŠ”", "ë‚´ê°€", "ë§Œ", 
        "ìˆ˜", "ê²ƒì€", "ìœ„í•œ", "ì•„ë‹ˆë¼", "ìš°ë¦¬ê°€", "ìˆìŠµë‹ˆë‹¤", "ì–´ë–¤", 'ê°„', 'ë¶€', 'ê²ƒ', 'ë¬´ì—‡', 
        'ê²ƒ', 'ì±…', 'ë™ì•ˆ', 'ì²«', 'ì•ˆ', 'ë¥¼', 'ë“±', 'ê³³', 'ì™œ', 'ì˜', 'ì´', 'ì‘', 
        'ê·¸', 'ê°„', 'ìœ„', 'ìš°ë¦¬', 'ë‚˜', 'ë…„', 'ì˜¨', 'ëˆ„êµ¬', 'ì•Œ', 'ë•Œ', 'ê¶Œ', 'ëŒì•„ì™”', "ì°¾ì•„ì™”", "ë°ì•„ì¡Œ", 
        'ì´ì•¼ê¸°', 'ë•Œë¬¸', 'ì†', 'íƒœì–´ë‚¬', 'ë°›ì•„ë“¤ì˜€', "ì¼ì–´ë‚¬", 'í’€ì–´ì¡Œ', 'ë§Œë‚¬', "í•´", "ì„¤ì„œ", "ì˜¬ë", 
        "ë² ìŠ¤íŠ¸ì…€ëŸ¬" , "ì¶œê°„", "ë§", "ë¶„ì•¼", "ìµœê³ ", "ë§", "ë…ì", "ë™ì•ˆ", "ë•Œ", "ì¼", "ì§‘", "ë‚ ", 'ìƒê°', 'ë¬¸ì œ', 
        "ìˆœê°„", "ì´í›„", "ì‹œê°„", "ê¸°ë¡", "ì´ìƒ", "ìµœì´ˆ", 'ë¬´ì—‡', 'íŒ©', 'ì…˜', 'ê°„', 'ë‹¨', 'ë²ˆ', 'ë ˆ', 'ê¸°', 'í›„', 'ê·¸', 'ì¥' 
        'í•´', 'ì£¼ìš”', 'ã„±', 'ã…‚', 'ã……', 'ã…', 'ê¶Œì€', 'ì€', 'ì¥', 'ëª°', 'íŠœë¸Œ', 'ìš°ë¦¬', 'ì´ì•¼ê¸°', 'ìì‹ ', 'ë§', 'ë¦°', 'í˜¸', 'ì '
        'ìœ ', 'ë“±', 'ë…„', 'ê·¸', 'íŒŒ', 'ë°©', 'ì‘ê°€', 'ì´ì•¼ê¸°', 'ë…ì', 'ì¶œê°„', 'ë‚˜', 'ìì‹ ', 'ë­', 'í•œêµ­', 'ëŒ€í•œë¯¼êµ­', 'ê¸€', 'ìŠ¤ìŠ¤', 
        "ì…˜", "ì—ë””", "ë¶€", "ë‚˜", "í•˜ë‚˜", "ì‚´", "ì¼", "ê¶Œ", "ì„¸", "ë‘", "ì´ì œ", "ë•Œ", 'ì‡ìŠµë‹ˆë‹¤', 'ê°€', 'í‹±ë‚«', 'í¸', 'ê°œì •', 'íŒ', 'íŠ¹ì§•',
        "ë²ˆ", "ê´€", "ì´", "í‹°ë‹ˆ", "ì‹¤ë¡", "ê°„", "ìŠ¤", "ì‘", "íšŒ", "ìê¸°", "ê³³", 'ê²ƒì´', 'ì¹˜', 'ì§€ê¸ˆ', 'ë‹¹ì‹ ', 'ëª»', 'ì§•', 'ë‹ˆë¡œ', 'ê³¼', 'ê³ '
        'ë‹¨', 'ì•„ì´íˆë§Œì€', 'êµ­ë¦½ì–´ë¦°ì´ì²­ì†Œë…„ë„ì„œê´€', 'ë„ì„œ', 'ê¶Œì¥ë„ì„œ', 'ë‚˜' 'ì‹œê°„' 'ê¸€', 'ë•Œ', "ì´ê²ƒ", 'ë…ì', 'ë‹¤ë¥¸', 'ì§ˆë¬¸', 'ì—ì„¸ì´', 'ëª©', 'ê¹ƒ', 'ì„œ',
        'ê·¸ë¦¼', 'ì„¸ìƒ', 'ì˜¤ëŠ˜', 'í•˜ë£¨', 'ë¬¸ì¥', 'ì„ ì •', 'ë²ˆì—­', 'ëŒ€í‘œ', 'ì›”', 'ê¸°', 'ì €ì', 'ëª¨ë‘', 'ë‹¤ì‹œ', 'ë˜', 'ë“œì‰', 'ë¶', 'ë¬´', 'ì¼ìƒ', 'ë²•', 'ëˆˆ', 'ë§¤ì¼', 'ì¤„']

# ë¶ˆìš©ì–´ ì œê±° í•¨ìˆ˜
# @st.cache
def remove_stop(text):
        token = text.split(" ")
        stops = stopwords
        meaningful_words = [w for w in token if  w not in stops]
        return ' '.join(meaningful_words)
    
# ë¶ˆìš©ì–´ í•¨ìˆ˜ ì ìš©
df["ì„¤ëª…"] = df["ì„¤ëª…"].map(lambda x : remove_stop(x))
df["ì„¤ëª…"] = df['ì„¤ëª…'].str.replace("[\s]+", " ", regex=True)
df['ì„¤ëª…'] = df['ì„¤ëª…'].str.replace("['ê¸€']", '', regex=True)
df['ì„¤ëª…'] = df['ì„¤ëª…'].str.replace("['ê´€ê³„']", '', regex=True)
df['ì„¤ëª…'] = df['ì„¤ëª…'].str.replace("['ë°”ë¡œ']", '', regex=True)
df['ì„¤ëª…'] = df['ì„¤ëª…'].str.replace("['ìœ„']", '', regex=True)
df['ì„¤ëª…'] = df['ì„¤ëª…'].str.replace("['ìœ íŠœë¸Œ']", '', regex=True)
df["ì„¤ëª…"] = df['ì„¤ëª…'].str.replace("[\s]+", " ", regex=True)

# í† í”½ëª¨ë¸ë§
# @st.cache
class TopicModeling :
    # ìµœì†Œ 5ê°œ ì´ìƒ ì„¤ëª…ì— ë“±ì¥í•˜ê³ 
    # ì „ì²´ ì¶œí˜„ë¹ˆë„ëŠ” 15ê°œ ì´ìƒì¸ ë‹¨ì–´ë§Œ ì‚¬ìš©
    def __init__(self, df, k, min_df=10, min_cf=10) : 
        self.df = df
        self.k = k  # í† í”½ì˜ ê°œìˆ˜
        self.min_df = min_df
        self.min_cf = min_cf

    def LDA(self) :

        LDA_model = tp.LDAModel(k=self.k, min_df=self.min_df, min_cf=self.min_cf, tw=tp.TermWeight.ONE, rm_top=1,
                                alpha=0.1, eta=0.01)
        for token in self.df['ì„¤ëª…'].str.split(' '):
            LDA_model.add_doc(token)

        return LDA_model

    def train(self, LDA_model):
        LDA_model.train(0)
        print("Num docs :", len(LDA_model.docs), ', vocab_size:', LDA_model.num_vocabs, ', Num words:', LDA_model.num_words)
        print('Removed top word: ', LDA_model.removed_top_words)
        print('Training...', file=sys.stderr, flush=True) 
        for i in range(0, 1000, 100):
            LDA_model.train(100)
            print("Iteration : {}\tLog-likelihood: {}".format(i, LDA_model.ll_per_word))

    def result(self, LDA_model):
        for i in range(LDA_model.k):
            res = LDA_model.get_topic_words(i, top_n=10)
            print("Topic #{}".format(i), end='\t')
            print(', '.join(w for w, p in res))
    
    def get_coherence(self, LDA_model):
        coherence = tp.coherence
        score = coherence.Coherence(LDA_model).get_score()
        perplexity = LDA_model.perplexity
        print("topic ê°œìˆ˜ : ", self.k, "| ì‚¬ìš©ëœ vocab ìˆ˜ : ", len(LDA_model.used_vocabs) , "| Coherence ì ìˆ˜ : ", score, 
                "| Perplexity ì ìˆ˜ : ", perplexity)
        return score, perplexity
    
# íŠ¸ë ˆì´ë‹
lda = TopicModeling(df, 20)
model = lda.LDA()
lda.train(model)
len(model.used_vocabs)
lda.result(model)
lda.get_coherence(model)

# í† í”½ ê´€ë ¨ í•­ëª© ì¶”ê°€
# df['top_topic'] = 0
# df['topic_dist'] = 0
# df['topic_words'] = 0

# for i in tqdm(range(len(model.docs))):
#     df["topic_dist"][i] = model.docs[i].get_topics(top_n=4)
#     df['top_topic'][i] = model.docs[i].get_topics()[0][0]
#     res = model.get_topic_words(df['top_topic'][i], top_n=5)
#     df["topic_words"][i] = ' '.join(w for w, p in res)
    
st.dataframe(df[['ìƒí’ˆëª…','top_topic','topic_words']])

# countplot ì‹œê°í™”
# fig, ax = plt.subplots(figsize=(20,10))
# sns.countplot(df[df['ê´€ë¦¬ë¶„ë¥˜']=='ì—ì„¸ì´'], x='top_topic').set_title('ì—ì„¸ì´ ë¶„ì•¼ í† í”½ ë¶„í¬');
# st.pyplot(fig)

